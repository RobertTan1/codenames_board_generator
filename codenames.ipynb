{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "narrow-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sp\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### SETUP ############################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "precious-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 400 words from codenames\n",
    "codenames_words = pd.read_csv('codenames_words.csv',index_col=False)\n",
    "codenames_words = codenames_words['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "differential-kingdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl (778.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 778.8 MB 45 kB/s s eta 0:00:01     |█████████████████████▎          | 517.2 MB 11.2 MB/s eta 0:00:24\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from en-core-web-lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.60.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (20.9)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (54.1.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.20.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/roberttan/Documents/python virtual env/venv/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.0.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Download word vectors if not downloaded.\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# Load word vectors from spacy\n",
    "nlp = sp.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### PREPROCESS #######################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "associate-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word permutation matrix, will be used to make semantic distance dataset\n",
    "ab = list(itertools.product(codenames_words,codenames_words))\n",
    "codenames_pairs = pd.DataFrame(ab,columns=(\"source\",\"destination\"))\n",
    "codenames_pairs['equi'] = codenames_pairs.source != codenames_pairs.destination\n",
    "codenames_pairs = codenames_pairs[codenames_pairs['equi']]\n",
    "codenames_pairs['mixed_string'] = codenames_pairs['source'] + codenames_pairs['destination']\n",
    "\n",
    "# Cleanup\n",
    "codenames_pairs.reset_index(inplace=True)\n",
    "codenames_pairs.drop(columns = 'index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "challenging-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159600/159600 [03:19<00:00, 801.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get rid of duplicates to save time\n",
    "for i in tqdm(range(0, len(codenames_pairs))):\n",
    "    codenames_pairs['mixed_string'].iloc[i] = ''.join(sorted(codenames_pairs['mixed_string'].iloc[i]))\n",
    "\n",
    "codenames_pairs.drop_duplicates(subset='mixed_string', keep=\"last\", inplace = True)\n",
    "codenames_pairs.drop(columns = ['equi'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### NLP ##############################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tight-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-calculated distances, skip to Algo section\n",
    "codenames_pairs = pd.read_csv('codenames_with_distances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP time\n",
    "codenames_pairs['semantic_proximity'] = 0.0\n",
    "\n",
    "for i in tqdm(range(0, len(codenames_pairs))):\n",
    "    t1 = nlp(codenames_pairs.source.iloc[i])\n",
    "    t2 = nlp(codenames_pairs.destination.iloc[i])\n",
    "    codenames_pairs['semantic_proximity'].iloc[i] = t1.similarity(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "codenames_pairs.to_csv('codenames_with_distances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### ALGO #############################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3 * 10^39 possible board combinations so doing it completely would be impractical...\n",
    "# We have to use a heuristic\n",
    "\n",
    "# Heuristic: two words that have little or a lot to do with each other have bias. We leverage this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interesting-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distance(word_distance, iterations = 5):\n",
    "    \"\"\"\n",
    "    Returns a list of 25 words for the game of Codenames.\n",
    "    Choose how close you want your words to be!\n",
    "    \"\"\"\n",
    "    if word_distance < 1 or word_distance > 21:\n",
    "        raise('Word distance must be between 1 and 21')\n",
    "\n",
    "    # Create dataframe to store list values and total score\n",
    "    results_frame = pd.DataFrame(columns = ['word_board', 'score'])\n",
    "    \n",
    "    # Algo start\n",
    "    for i in tqdm(range(1,iterations+1,1), desc=\"Cooking Words\"):\n",
    "        seed_subset = codenames_pairs[(codenames_pairs.semantic_proximity <= codenames_pairs.semantic_proximity.quantile([1-(word_distance-1)/21]).iloc[0]) & (codenames_pairs.semantic_proximity >= codenames_pairs.semantic_proximity.quantile([1-word_distance/21]).iloc[0])]\n",
    "        seed_pair = seed_subset.iloc[random.randint(0,len(seed_subset))]\n",
    "        chosen_values = []\n",
    "        chosen_values.append(seed_pair.source)\n",
    "        chosen_values.append(seed_pair.destination)\n",
    "        \n",
    "        if word_distance > 10:\n",
    "            seed_pair = seed_subset.iloc[random.randint(0,len(seed_subset))]\n",
    "            chosen_values.append(seed_pair.source)\n",
    "            chosen_values.append(seed_pair.destination)\n",
    "        \n",
    "        chosen_values = list(set(chosen_values))\n",
    "\n",
    "        while len(chosen_values) < 25:\n",
    "            # get a subset of the first word's pairs to choose from\n",
    "            leg_1 = codenames_pairs[(codenames_pairs['source'] == chosen_values[-2]) | (codenames_pairs['destination'] == chosen_values[-2])]\n",
    "            # get the best candidates in this subset\n",
    "            if word_distance > 10:\n",
    "                dirty_values = list(leg_1.sort_values(by=['semantic_proximity']).head(45).source) + list(leg_1.sort_values(by=['semantic_proximity']).head(45).destination)\n",
    "            else:\n",
    "                dirty_values = list(leg_1.sort_values(by=['semantic_proximity'], ascending=False).head(45).source) + list(leg_1.sort_values(by=['semantic_proximity'], ascending=False).head(45).destination)\n",
    "            # filter out already existing values\n",
    "            search_values = [x for x in dirty_values if x not in chosen_values]\n",
    "\n",
    "            # Create subset of the second word\n",
    "            leg_2 = codenames_pairs[(codenames_pairs['source'] == chosen_values[-1]) | (codenames_pairs['destination'] == chosen_values[-1])]\n",
    "            # Get the best candidate out of the candidates and append to chosen_values\n",
    "            if word_distance > 10:\n",
    "                leg_2_list = list(leg_2[(leg_2['source'].isin(search_values)) | (leg_2['destination'].isin(search_values))].sort_values(by='semantic_proximity').iloc[random.randint(word_distance,round(word_distance*((1+math.sqrt(5))/2), 0))][['source', 'destination']])\n",
    "            else:\n",
    "                leg_2_list = list(leg_2[(leg_2['source'].isin(search_values)) | (leg_2['destination'].isin(search_values))].sort_values(by='semantic_proximity', ascending = False).iloc[random.randint(word_distance, round(word_distance*((1+math.sqrt(5))/2), 0))][['source', 'destination']])\n",
    "            leg_2_list = [x for x in leg_2_list if x not in chosen_values]\n",
    "            chosen_values.extend(leg_2_list)\n",
    "\n",
    "        results_frame = results_frame.append({'word_board' : chosen_values,\n",
    "                        'score' : 0} , \n",
    "                        ignore_index=True)\n",
    "    \n",
    "    # Tabulate score \n",
    "    if iterations == 1:\n",
    "        pass\n",
    "    else:\n",
    "        for i in tqdm(range(0,len(results_frame)), desc=\"Garnishing\"):\n",
    "            ab = list(itertools.product(results_frame.word_board[i],results_frame.word_board[i]))\n",
    "            score_pairs = pd.DataFrame(ab,columns=(\"source\",\"destination\"))\n",
    "            score_pairs['equi'] = score_pairs.source != score_pairs.destination\n",
    "            score_pairs = score_pairs[score_pairs['equi']]\n",
    "            score_pairs['mixed_string'] = score_pairs['source'] + score_pairs['destination']\n",
    "\n",
    "            for j in range(0, len(score_pairs)):\n",
    "                score_pairs['mixed_string'].iloc[j] = ''.join(sorted(score_pairs['mixed_string'].iloc[j]))\n",
    "\n",
    "            score_pairs.drop_duplicates(subset='mixed_string', keep=\"last\", inplace = True)\n",
    "            score_pairs.drop(columns = ['equi'], inplace = True)\n",
    "\n",
    "            t = pd.merge(score_pairs, codenames_pairs, how='left', on=['mixed_string'])\n",
    "            total = sum(t.semantic_proximity)\n",
    "\n",
    "            if sum(t.semantic_proximity.isnull().values.ravel()) > 0:\n",
    "                print(\"One or more missing semantic proximity values. Check if record exists between score_pairs and codenames_words\")\n",
    "                break\n",
    "            results_frame.iat[i, 1] = total\n",
    "    \n",
    "    # Return list\n",
    "    if word_distance > 10:\n",
    "        the_result = results_frame.sort_values(by='score').iloc[0].word_board\n",
    "    else:\n",
    "        the_result = results_frame.sort_values(by='score', ascending = False).iloc[0].word_board\n",
    "    random.shuffle(the_result)\n",
    "    return '[%s]' % ', '.join(map(str, the_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "gentle-lecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cooking Words: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
      "Garnishing: 100%|██████████| 20/20 [00:04<00:00,  4.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[Venus, Page, Smoothie, Coach, Avalanche, Blacksmith, Kilt, Patient, Sumo, Goldilocks, Mill, Tattoo, Crusader, Hit, Reindeer, Beam, Joker, Director, Book, Saloon, Mile, Manicure, Meter, Disk, Newton]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_distance(21, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-evanescence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
