{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "inclusive-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sp\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### SETUP ############################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 400 words from codenames\n",
    "codenames_words = pd.read_csv('codenames_words.csv',index_col=False)\n",
    "codenames_words = codenames_words['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors from spacy\n",
    "nlp = sp.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### PREPROCESS #######################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word permutation matrix, will be used to make semantic distance dataset\n",
    "ab = list(itertools.product(codenames_words,codenames_words))\n",
    "codenames_pairs = pd.DataFrame(ab,columns=(\"source\",\"destination\"))\n",
    "codenames_pairs['equi'] = codenames_pairs.source != codenames_pairs.destination\n",
    "codenames_pairs = codenames_pairs[codenames_pairs['equi']]\n",
    "codenames_pairs['mixed_string'] = codenames_pairs['source'] + codenames_pairs['destination']\n",
    "\n",
    "# Cleanup\n",
    "codenames_pairs.reset_index(inplace=True)\n",
    "codenames_pairs.drop(columns = 'index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of duplicates to save time\n",
    "for i in tqdm(range(0, len(codenames_pairs))):\n",
    "    codenames_pairs['mixed_string'].iloc[i] = ''.join(sorted(codenames_pairs['mixed_string'].iloc[i]))\n",
    "\n",
    "codenames_pairs.drop_duplicates(subset='mixed_string', keep=\"last\", inplace = True)\n",
    "codenames_pairs.drop(columns = ['equi'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### NLP ##############################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP time\n",
    "codenames_pairs['semantic_proximity'] = 0.0\n",
    "\n",
    "for i in tqdm(range(0, len(codenames_pairs))):\n",
    "    t1 = nlp(codenames_pairs.source.iloc[i])\n",
    "    t2 = nlp(codenames_pairs.destination.iloc[i])\n",
    "    codenames_pairs['semantic_proximity'].iloc[i] = t1.similarity(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "codenames_pairs.to_csv('codenames_with_distances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### ALGO #############################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3 * 10^39 possible board combinations so doing it completely would be impractical...\n",
    "# We have to use a heuristic\n",
    "\n",
    "# Heuristic: two words that have little or a lot to do with each other have bias. We leverage this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "romantic-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distance(word_distance, iterations = 5):\n",
    "    \"\"\"\n",
    "    Returns a list of 25 words for the game of Codenames.\n",
    "    Choose how close you want your words to be!\n",
    "    \"\"\"\n",
    "    if word_distance < 1 or word_distance > 21:\n",
    "        raise('Word distance must be between 1 and 21')\n",
    "\n",
    "    # Create dataframe to store list values and total score\n",
    "    results_frame = pd.DataFrame(columns = ['word_board', 'score'])\n",
    "    \n",
    "    # Algo start\n",
    "    for i in tqdm(range(1,iterations+1,1), desc=\"Cooking Words\"):\n",
    "        seed_subset = codenames_pairs[(codenames_pairs.semantic_proximity <= codenames_pairs.semantic_proximity.quantile([1-(word_distance-1)/21]).iloc[0]) & (codenames_pairs.semantic_proximity >= codenames_pairs.semantic_proximity.quantile([1-word_distance/21]).iloc[0])]\n",
    "        seed_pair = seed_subset.iloc[random.randint(0,len(seed_subset))]\n",
    "        chosen_values = []\n",
    "        chosen_values.append(seed_pair.source)\n",
    "        chosen_values.append(seed_pair.destination)\n",
    "        \n",
    "        if word_distance > 10:\n",
    "            seed_pair = seed_subset.iloc[random.randint(0,len(seed_subset))]\n",
    "            chosen_values.append(seed_pair.source)\n",
    "            chosen_values.append(seed_pair.destination)\n",
    "        \n",
    "        chosen_values = list(set(chosen_values))\n",
    "\n",
    "        while len(chosen_values) < 25:\n",
    "            # get a subset of the first word's pairs to choose from\n",
    "            leg_1 = codenames_pairs[(codenames_pairs['source'] == chosen_values[-2]) | (codenames_pairs['destination'] == chosen_values[-2])]\n",
    "            # get the best candidates in this subset\n",
    "            if word_distance > 10:\n",
    "                dirty_values = list(leg_1.sort_values(by=['semantic_proximity']).head(45).source) + list(leg_1.sort_values(by=['semantic_proximity']).head(45).destination)\n",
    "            else:\n",
    "                dirty_values = list(leg_1.sort_values(by=['semantic_proximity'], ascending=False).head(45).source) + list(leg_1.sort_values(by=['semantic_proximity'], ascending=False).head(45).destination)\n",
    "            # filter out already existing values\n",
    "            search_values = [x for x in dirty_values if x not in chosen_values]\n",
    "\n",
    "            # Create subset of the second word\n",
    "            leg_2 = codenames_pairs[(codenames_pairs['source'] == chosen_values[-1]) | (codenames_pairs['destination'] == chosen_values[-1])]\n",
    "            # Get the best candidate out of the candidates and append to chosen_values\n",
    "            if word_distance > 10:\n",
    "                leg_2_list = list(leg_2[(leg_2['source'].isin(search_values)) | (leg_2['destination'].isin(search_values))].sort_values(by='semantic_proximity').iloc[random.randint(word_distance,round(word_distance*((1+math.sqrt(5))/2), 0))][['source', 'destination']])\n",
    "            else:\n",
    "                leg_2_list = list(leg_2[(leg_2['source'].isin(search_values)) | (leg_2['destination'].isin(search_values))].sort_values(by='semantic_proximity', ascending = False).iloc[random.randint(word_distance, round(word_distance*((1+math.sqrt(5))/2), 0))][['source', 'destination']])\n",
    "            leg_2_list = [x for x in leg_2_list if x not in chosen_values]\n",
    "            chosen_values.extend(leg_2_list)\n",
    "\n",
    "        results_frame = results_frame.append({'word_board' : chosen_values,\n",
    "                        'score' : 0} , \n",
    "                        ignore_index=True)\n",
    "    \n",
    "    # Tabulate score \n",
    "    if iterations == 1:\n",
    "        pass\n",
    "    else:\n",
    "        for i in tqdm(range(0,len(results_frame)), desc=\"Garnishing\"):\n",
    "            ab = list(itertools.product(results_frame.word_board[i],results_frame.word_board[i]))\n",
    "            score_pairs = pd.DataFrame(ab,columns=(\"source\",\"destination\"))\n",
    "            score_pairs['equi'] = score_pairs.source != score_pairs.destination\n",
    "            score_pairs = score_pairs[score_pairs['equi']]\n",
    "            score_pairs['mixed_string'] = score_pairs['source'] + score_pairs['destination']\n",
    "\n",
    "            for j in range(0, len(score_pairs)):\n",
    "                score_pairs['mixed_string'].iloc[j] = ''.join(sorted(score_pairs['mixed_string'].iloc[j]))\n",
    "\n",
    "            score_pairs.drop_duplicates(subset='mixed_string', keep=\"last\", inplace = True)\n",
    "            score_pairs.drop(columns = ['equi'], inplace = True)\n",
    "\n",
    "            t = pd.merge(score_pairs, codenames_pairs, how='left', on=['mixed_string'])\n",
    "            total = sum(t.semantic_proximity)\n",
    "\n",
    "            if sum(t.semantic_proximity.isnull().values.ravel()) > 0:\n",
    "                print(\"One or more missing semantic proximity values. Check if record exists between score_pairs and codenames_words\")\n",
    "                break\n",
    "            results_frame.iat[i, 1] = total\n",
    "    \n",
    "    # Return list\n",
    "    if word_distance > 10:\n",
    "        the_result = results_frame.sort_values(by='score').iloc[0].word_board\n",
    "    else:\n",
    "        the_result = results_frame.sort_values(by='score', ascending = False).iloc[0].word_board\n",
    "    random.shuffle(the_result)\n",
    "    return '[%s]' % ', '.join(map(str, the_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-history",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cooking Words:  60%|██████    | 3/5 [00:02<00:01,  1.07it/s]"
     ]
    }
   ],
   "source": [
    "generate_distance(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-tuner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
