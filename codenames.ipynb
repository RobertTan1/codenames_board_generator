{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "average-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sp\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### SETUP ############################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stunning-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 400 words from codenames\n",
    "codenames_words = pd.read_csv('codenames_words.csv',index_col=False)\n",
    "codenames_words = codenames_words['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "french-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors from spacy\n",
    "nlp = sp.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### PREPROCESS #######################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "marked-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word permutation matrix, will be used to make semantic distance dataset\n",
    "ab = list(itertools.product(codenames_words,codenames_words))\n",
    "codenames_pairs = pd.DataFrame(ab,columns=(\"source\",\"destination\"))\n",
    "codenames_pairs['equi'] = codenames_pairs.source != codenames_pairs.destination\n",
    "codenames_pairs = codenames_pairs[codenames_pairs['equi']]\n",
    "codenames_pairs['mixed_string'] = codenames_pairs['source'] + codenames_pairs['destination']\n",
    "\n",
    "# Cleanup\n",
    "codenames_pairs.reset_index(inplace=True)\n",
    "codenames_pairs.drop(columns = 'index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "split-array",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/78960 [00:00<?, ?it/s]/Users/robert1tan/Documents/jup1/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "100%|██████████| 78960/78960 [01:42<00:00, 769.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get rid of duplicates to save time\n",
    "for i in tqdm(range(0, len(codenames_pairs))):\n",
    "    codenames_pairs['mixed_string'].iloc[i] = ''.join(sorted(codenames_pairs['mixed_string'].iloc[i]))\n",
    "\n",
    "completed_sort_backup = codenames_pairs.copy()\n",
    "codenames_pairs.drop_duplicates(subset='mixed_string', keep=\"last\", inplace = True)\n",
    "codenames_pairs.drop(columns = ['equi'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### NLP ##############################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "social-typing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79358 [00:00<?, ?it/s]/Users/robert1tan/Documents/jup1/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "100%|██████████| 79358/79358 [25:44<00:00, 51.40it/s]  \n"
     ]
    }
   ],
   "source": [
    "# NLP time\n",
    "codenames_pairs['semantic_proximity'] = 0.0\n",
    "\n",
    "for i in tqdm(range(0, len(codenames_pairs))):\n",
    "    t1 = nlp(codenames_pairs.source.iloc[i])\n",
    "    t2 = nlp(codenames_pairs.destination.iloc[i])\n",
    "    codenames_pairs['semantic_proximity'].iloc[i] = t1.similarity(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "rubber-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "codenames_pairs.to_csv('codenames_with_distances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "################################### ALGO #############################\n",
    "######################################################################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3 * 10^39 possible board combinations so doing it completely would be impractical...\n",
    "# We have to use a heuristic\n",
    "\n",
    "# Heuristic: two words that have little to do with each other likely do not share words with high proximity\n",
    "# So we do the following: start with 2 seed words that are very far apart...\n",
    "# Then from that subset: pick the 30 least connected words to the first word, and select the least connected word with the second word\n",
    "    # will have to filter out words already in set\n",
    "    # do this until 25 are reached\n",
    "    # store as a list for further computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "spread-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_codename_words(word_distance, iterations = 20):\n",
    "    \"\"\"\n",
    "    Returns a list of 25 words for the game of Codenames.\n",
    "    Choose how close you want your words to be!\n",
    "    \"\"\"\n",
    "    if word_distance < 1 or word_distance > 11:\n",
    "        raise('Word distance must be between 1 and 11')\n",
    "\n",
    "    # Create dataframe to store list values and total score\n",
    "    results_frame = pd.DataFrame(columns = ['word_board', 'score'])\n",
    "    \n",
    "    # Algo start\n",
    "    for i in tqdm(range(1,iterations+1,1), desc=\"Cooking Words\"):\n",
    "        seed_subset = codenames_pairs[(codenames_pairs.semantic_proximity <= codenames_pairs.semantic_proximity.quantile([1-(word_distance-1)/11]).iloc[0]) & (codenames_pairs.semantic_proximity >= codenames_pairs.semantic_proximity.quantile([1-word_distance/11]).iloc[0])]\n",
    "        seed_pair = seed_subset.iloc[random.randint(0,len(seed_subset))]\n",
    "        chosen_values = []\n",
    "        # take the top 3 values from leg 1, then find the score in leg_2, get the lowest value from leg_2\n",
    "        chosen_values.append(seed_pair.source)\n",
    "        chosen_values.append(seed_pair.destination)\n",
    "\n",
    "        while len(chosen_values) < 25:\n",
    "            # get a subset of the first word's pairs to choose from\n",
    "            leg_1 = codenames_pairs[(codenames_pairs['source'] == chosen_values[-2]) | (codenames_pairs['destination'] == chosen_values[-2])]\n",
    "            # get the best 3 candidates in this subset\n",
    "            if word_distance > 6:\n",
    "                dirty_values = list(leg_1.sort_values(by=['semantic_proximity']).head(45).source) + list(leg_1.sort_values(by=['semantic_proximity']).head(45).destination)\n",
    "            else:\n",
    "                dirty_values = list(leg_1.sort_values(by=['semantic_proximity'], ascending=False).head(45).source) + list(leg_1.sort_values(by=['semantic_proximity'], ascending=False).head(45).destination)\n",
    "            # filter out already existing values\n",
    "            search_values = [x for x in dirty_values if x not in chosen_values]\n",
    "\n",
    "            # Create subset of the second word\n",
    "            leg_2 = codenames_pairs[(codenames_pairs['source'] == chosen_values[-1]) | (codenames_pairs['destination'] == chosen_values[-1])]\n",
    "            # Get the best candidate out of the 3 candidates and append to chosen_values\n",
    "            if word_distance > 6:\n",
    "                leg_2_list = list(leg_2[(leg_2['source'].isin(search_values)) | (leg_2['destination'].isin(search_values))].sort_values(by='semantic_proximity').iloc[10][['source', 'destination']])\n",
    "            else:\n",
    "                leg_2_list = list(leg_2[(leg_2['source'].isin(search_values)) | (leg_2['destination'].isin(search_values))].sort_values(by='semantic_proximity', ascending = False).iloc[10][['source', 'destination']])\n",
    "            leg_2_list = [x for x in leg_2_list if x not in chosen_values]\n",
    "            chosen_values.extend(leg_2_list)\n",
    "\n",
    "        results_frame = results_frame.append({'word_board' : chosen_values,\n",
    "                        'score' : 0} , \n",
    "                        ignore_index=True)\n",
    "    \n",
    "    # Tabulate score \n",
    "    for i in tqdm(range(0,len(results_frame)), desc=\"Garnishing\"):\n",
    "        ab = list(itertools.product(results_frame.word_board[i],results_frame.word_board[i]))\n",
    "        score_pairs = pd.DataFrame(ab,columns=(\"source\",\"destination\"))\n",
    "        score_pairs['equi'] = score_pairs.source != score_pairs.destination\n",
    "        score_pairs = score_pairs[score_pairs['equi']]\n",
    "        score_pairs['mixed_string'] = score_pairs['source'] + score_pairs['destination']\n",
    "\n",
    "        for j in range(0, len(score_pairs)):\n",
    "            score_pairs['mixed_string'].iloc[j] = ''.join(sorted(score_pairs['mixed_string'].iloc[j]))\n",
    "\n",
    "        score_pairs.drop_duplicates(subset='mixed_string', keep=\"last\", inplace = True)\n",
    "        score_pairs.drop(columns = ['equi'], inplace = True)\n",
    "\n",
    "        t = pd.merge(score_pairs, codenames_pairs, how='left', on=['mixed_string'])\n",
    "        total = sum(t.semantic_proximity)\n",
    "\n",
    "        if sum(t.semantic_proximity.isnull().values.ravel()) > 0:\n",
    "            print(\"One or more missing semantic proximity values. Check if record exists between score_pairs and codenames_words\")\n",
    "            break\n",
    "        results_frame.iat[i, 1] = total\n",
    "    \n",
    "    # Return list\n",
    "    the_result = results_frame.sort_values(by='score').iloc[int(round(iterations/2,0))].word_board\n",
    "    random.shuffle(the_result)\n",
    "    return the_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "unauthorized-alabama",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:28<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1,iters+1,1)):\n",
    "    seed_subset = codenames_pairs[(codenames_pairs.semantic_proximity >= codenames_pairs.semantic_proximity.quantile([word_distance/11-1/11]).iloc[0]) & (codenames_pairs.semantic_proximity <= codenames_pairs.semantic_proximity.quantile([(word_distance + 1)/11-1/11]).iloc[0])]\n",
    "    seed_pair = seed_subset.iloc[random.randint(0,len(seed_subset))]\n",
    "    chosen_values = []\n",
    "    # take the top 3 values from leg 1, then find the score in leg_2, get the lowest value from leg_2\n",
    "    chosen_values.append(seed_pair.source)\n",
    "    chosen_values.append(seed_pair.destination)\n",
    "\n",
    "\n",
    "    while len(chosen_values) < 25:\n",
    "        # get a subset of the first word's pairs to choose from\n",
    "        leg_1 = codenames_pairs[(codenames_pairs['source'] == chosen_values[-2]) | (codenames_pairs['destination'] == chosen_values[-2])]\n",
    "        # get the best 3 candidates in this subset\n",
    "        if word_distance < 6:\n",
    "            dirty_values = list(leg_1.sort_values(by=['semantic_proximity']).head(30).source) + list(leg_1.sort_values(by=['semantic_proximity']).head(30).destination)\n",
    "        else:\n",
    "            dirty_values = list(leg_1.sort_values(by=['semantic_proximity'], ascending=False).head(30).source) + list(leg_1.sort_values(by=['semantic_proximity'], ascending=False).head(30).destination)\n",
    "        # filter out already existing values\n",
    "        search_values = [x for x in dirty_values if x not in chosen_values]\n",
    "\n",
    "        # Create subset of the second word\n",
    "        leg_2 = codenames_pairs[(codenames_pairs['source'] == chosen_values[-1]) | (codenames_pairs['destination'] == chosen_values[-1])]\n",
    "        # Get the best candidate out of the 3 candidates and append to chosen_values\n",
    "        if word_distance < 6:\n",
    "            leg_2_list = list(leg_2[(leg_2['source'].isin(search_values)) | (leg_2['destination'].isin(search_values))].sort_values(by='semantic_proximity').iloc[0][['source', 'destination']])\n",
    "        else:\n",
    "            leg_2_list = list(leg_2[(leg_2['source'].isin(search_values)) | (leg_2['destination'].isin(search_values))].sort_values(by='semantic_proximity', ascending = False).iloc[0][['source', 'destination']])\n",
    "        leg_2_list = [x for x in leg_2_list if x not in chosen_values]\n",
    "        chosen_values.extend(leg_2_list)\n",
    "    \n",
    "    results_frame = results_frame.append({'word_board' : chosen_values,\n",
    "                    'score' : 0} , \n",
    "                    ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "dutch-graphics",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now tabulate the score\n",
    "# the score = total_proximity = sum of all semantic_proximity, the lower the more distant the board\n",
    "    # we do this by knowing each value pair, should be 300 pairs, then we just sum it\n",
    "    # can likely reuse matrix logic from earlier\n",
    "\n",
    "for i in tqdm(range(0,len(results_frame))):\n",
    "    ab = list(itertools.product(results_frame.word_board[i],results_frame.word_board[i]))\n",
    "    score_pairs = pd.DataFrame(ab,columns=(\"source\",\"destination\"))\n",
    "    score_pairs['equi'] = score_pairs.source != score_pairs.destination\n",
    "    score_pairs = score_pairs[score_pairs['equi']]\n",
    "    score_pairs['mixed_string'] = score_pairs['source'] + score_pairs['destination']\n",
    "\n",
    "    for j in range(0, len(score_pairs)):\n",
    "        score_pairs['mixed_string'].iloc[j] = ''.join(sorted(score_pairs['mixed_string'].iloc[j]))\n",
    "\n",
    "    score_pairs.drop_duplicates(subset='mixed_string', keep=\"last\", inplace = True)\n",
    "    score_pairs.drop(columns = ['equi'], inplace = True)\n",
    "\n",
    "    t = pd.merge(score_pairs, codenames_pairs, how='left', on=['mixed_string'])\n",
    "    total = sum(t.semantic_proximity)\n",
    "#     print(total)\n",
    "    if sum(t.semantic_proximity.isnull().values.ravel()) > 0:\n",
    "        print(\"One or more missing semantic proximity values. Check if record exists between score_pairs and codenames_words\")\n",
    "        break\n",
    "    results_frame.iat[i, 1] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-above",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# OK, we got to the finish line. We now hard inputted a list of words and then built an approximate algorithm to generate the hardest boards\n",
    "# Additional features:\n",
    "# Being able to choose \"Word Distance\" from 1 to 11\n",
    "# 1 means close 11 means far. That can just translate into percentiles so we don't even really have to change sorting logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "laughing-ground",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cooking Words: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n",
      "Garnishing: 100%|██████████| 2/2 [00:00<00:00,  5.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Puppet',\n",
       " 'Valentine',\n",
       " 'Bowl',\n",
       " 'Frog',\n",
       " 'Wizard',\n",
       " 'Bucket',\n",
       " 'Rice',\n",
       " 'Hammer',\n",
       " 'Cave',\n",
       " 'Mud',\n",
       " 'Easter',\n",
       " 'Snake',\n",
       " 'Walrus',\n",
       " 'Troll',\n",
       " 'Scarecrow',\n",
       " 'Big Ben',\n",
       " 'Zombie',\n",
       " 'Cow',\n",
       " 'Vampire',\n",
       " 'Trick',\n",
       " 'Elephant',\n",
       " 'Crow',\n",
       " 'Rainbow',\n",
       " 'Cowboy',\n",
       " 'Christmas']"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_codename_words(6, iterations = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
